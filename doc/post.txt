Why AI Keeps Forgetting Your Repo Rules When Editing Code

AI coding assistants are surprisingly good at writing code.
They are also surprisingly bad at remembering rules.

Not hard rules like “don’t break the build”, but soft yet critical ones:
	•	architectural constraints
	•	performance caveats
	•	concurrency rules
	•	“don’t touch this unless you understand X”

Most teams put these in README, design docs, or specs.
And yet, the moment an AI touches the code, those rules quietly disappear.

This post is about why that happens, and why most current solutions don’t fully solve it.

⸻

The False Assumption: “The AI Will Read the Spec”

A growing number of teams are adopting spec-driven or design-first workflows for AI coding:
	•	Write specs
	•	Ask the AI to follow them
	•	Review the diff

This is a huge improvement over prompt-only coding.
But it relies on one fragile assumption:

The AI will remember to read the spec at the right time.

In practice, this assumption breaks down in several ways:
	•	Context windows are limited
	•	Specs live far away from the code being edited
	•	The AI focuses on the local diff, not global intent
	•	On subsequent edits, earlier context silently drops

Even with perfect specs, enforcement is mostly voluntary.

⸻

Specs Define Rules, But They Don’t Enforce Them

Specs answer an important question:

What are the rules?

But they don’t answer another equally important one:

When are those rules applied?

Today, rule enforcement depends on:
	•	the prompt being correct
	•	the AI behaving responsibly
	•	the human reviewer catching violations

That works… until it doesn’t.

This is why teams experience a familiar pattern:
	1.	Rules are written clearly
	2.	AI follows them at first
	3.	Weeks later, during a small refactor, the rules are violated
	4.	Nobody notices until production

The rules didn’t change.
They were just not present at the moment of modification.

⸻

The Real Problem Is Triggering, Not Specification

After watching this pattern repeat, I think the core issue isn’t missing specs.

It’s this:

Rules are not attached to the code they constrain.

They live nearby, but not with the code.

For humans, that’s usually fine.
For AI, it’s a structural weakness.

AI tools operate locally:
	•	“Here is the file”
	•	“Here is the diff”
	•	“Make this change”

If the rules aren’t automatically pulled in at that moment, they effectively don’t exist.

⸻

A Different Mental Model: Rules Should Be Load-Bearing

In programming languages, we don’t rely on comments to enforce correctness.

We use:
	•	types
	•	borrow rules
	•	lifetimes
	•	runtime checks

These constraints are triggered when code is touched.

AI coding needs something similar:

Rules that activate when the code is edited, not when the prompt is written.

⸻

What If Rules Were Anchored to Code?

Instead of assuming the AI will:
	•	read the spec
	•	remember it
	•	apply it consistently

What if we reversed the flow?
	•	AI touches code
	•	Code reveals its attached rules
	•	Rules are injected into context automatically

Now enforcement is no longer optional.

The AI doesn’t need to “remember” anything.
It simply cannot modify the code without seeing the constraints.

⸻

From Spec-Driven to Trigger-Driven AI Coding

Specs are still essential.
They define intent, scope, and invariants.

But specs alone are passive.

What’s missing is a trigger mechanism:
	•	When this code changes → load these rules
	•	When this module is edited → surface these constraints

That’s the gap between knowing the rules and enforcing them at the right moment.

⸻

A Practical Experiment: Linggen

I’ve been experimenting with this idea in a tool called Linggen.

The core idea is simple:
	•	Rules live as versioned memory files
	•	Code contains lightweight anchors
	•	Editing anchored code automatically pulls in the relevant rules

No agent orchestration.
No black-box automation.
Just explicit, reviewable constraints that follow the code.

It doesn’t replace specs.
It complements them by solving the when problem.

⸻

Closing Thought

AI coding isn’t failing because models are careless.

It’s failing because our rules are:
	•	passive
	•	detached
	•	easy to forget

Specs answer what.
Anchors answer when.

Until rules become load-bearing, AI will keep forgetting them—no matter how well we write them.

⸻

Happy to hear how others are dealing with this problem.
If you’ve found better ways to enforce constraints with AI coding, I’d love to learn from them.